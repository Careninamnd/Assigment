{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IGcI6_rwm2g"
      },
      "source": [
        "# **Final Project 2**\n",
        "## **1. Perkenalan**\n",
        "\n",
        "Nama Anggota Kelompok :\n",
        "1. Muhammad Rafi Ramadhan / PYTN-KS12-002\n",
        "2. Aurisha Lutvinda Putrian / PYTN-KS12-014\n",
        "3. Carenina Amanda Putri / PYTN-KS12-017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nQ15IMDw8o7"
      },
      "source": [
        "\n",
        "\n",
        "### 1.1 Latar Belakang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6xOHncfUeJx"
      },
      "source": [
        "Data cuaca merupakan salah satu data yang penting dalam berbagai bidang seperti pertanian, transportasi, pariwisata, dan sebagainya. Namun, data cuaca tersebut seringkali tidak terorganisir dengan baik sehingga sulit untuk dianalisis. Oleh karena itu, perlu dilakukan pengolahan data agar data cuaca tersebut dapat dimanfaatkan dengan maksimal.\n",
        "\n",
        "Pada proyek ini, akan dilakukan pengolahan data cuaca dari dataset Weather Dataset Rattle Package yang berisi data cuaca di Australia. Data ini memiliki informasi mengenai cuaca pada hari ini dan prediksi cuaca untuk besok. Data tersebut mencakup variabel seperti suhu, kelembapan, tekanan udara, kecepatan angin, serta informasi apakah akan hujan pada hari ini dan besok.\n",
        "\n",
        "Dalam proyek ini, akan dilakukan beberapa tahap pengolahan data, seperti pembersihan data, eksplorasi data, serta pembuatan model prediksi untuk memprediksi apakah akan hujan pada besoknya. Dengan melakukan pengolahan data ini, diharapkan data cuaca yang tidak terorganisir dapat diolah menjadi informasi yang lebih berguna dan dapat membantu dalam berbagai bidang."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qesuQPTxJUg"
      },
      "source": [
        "## 1.2 Objektif\n",
        "- Mampu memahami konsep Classiﬁcation dengan Logistic Regression dan SVM\n",
        "-  Mampu mempersiapkan data untuk digunakan dalam model Logistic Regression\n",
        "dan SVM\n",
        "-  Mampu mengimplementasikan Logistic Regression dan SVM untuk membuat\n",
        "prediksi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpjjuu4pxkOt"
      },
      "source": [
        "## **2. Import Data**\n",
        "Pada bagian ini menggunakan import library untuk melakukan preproceesing, visualisasi dan modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhQVo9gFwN81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import folium\n",
        "import statistics\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.datasets import load_files, load_iris\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from pandas.plotting import scatter_matrix\n",
        "from nltk import FreqDist, NaiveBayesClassifier\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "%matplotlib inline\n",
        "pd.options.mode.chained_assignment = None\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLbS2oMWzRTn"
      },
      "source": [
        "## **3. Data Loading**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkExaKHTzWjc"
      },
      "source": [
        "## 3.1 Read Dataframe dari source CSV\n",
        "Memasukkan dataframe, dataframe yang digunakan kali ini adalah dataframe **Weather AUS**, yang kemudian menampilkan data teratas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASSlqNKMR2fn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "40d015a3-2d45-4065-e065-455ce5a975d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-54099f92-ad77-48d9-bef2-95726e7fba57\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-54099f92-ad77-48d9-bef2-95726e7fba57\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving weatherAUS.csv to weatherAUS (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "upload = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRXz-avZanMy"
      },
      "source": [
        "## 3.2 Menampilkan 5 data teratas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "jE-_vZoOapP6",
        "outputId": "a1a2ec2b-d93b-473d-bc40-b6aa1064f362"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-29a025bdd3e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset berisi {} baris dan {} kolom (atribut)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Dataset berisi {} baris dan {} kolom (atribut)\".format(df.shape[0], df.shape[1]))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDQ8TRCpazcp"
      },
      "source": [
        "### 3.3 Melihat data terbawah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXcTJ1tKa06a"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpu160ok_GFV"
      },
      "source": [
        "### 3.4 Melihat dimensi dari dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcyLS8im_eI8"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfzDnYt8CTMM"
      },
      "source": [
        "Dapat diketahui bahwa dataset yang digunakan terdiri dari 23 kolom dan 145460 baris."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzMv2C3sa78-"
      },
      "source": [
        "### 3.5 Melihat informasi kolom, baris, dan data type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQK4ML5qa_Yg"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTnJozdc7uSq"
      },
      "source": [
        "Keterangan:\n",
        "\n",
        "1.\t**Date** - tanggal hari itu\n",
        "2.\t**Location** - lokasi, nama kota di Australia\n",
        "3.\t**MinTemp** - temperatur terendah hari itu dalam celcius\n",
        "4.\t**MaxTemp** - temperatur tertinggi hari itu dalam celcius\n",
        "5.\t**Rainfall** - jumlah curah hujan hari itu dalam mm\n",
        "6.\t**Evaporation** - jumlah evaporasi dalam mm dari Class A pan selama 24 jam sebelum jam 9 pagi hari itu\n",
        "7.\t**Sunshine** - jumlah jam hari itu cerah dengan cahaya matahari\n",
        "8.\t**WindGustDir** - arah kecepatan angin yang paling tinggi selama 24 jam sebelum jam 12 malam hari itu\n",
        "9.\t**WindGustSpeed** - kecepatan angin yang paling tinggi dalam km/jam selama 24 jam sebelum jam 12 malam hari itu\n",
        "10.\t**WindDir9am** - arah angin jam 9 pagi\n",
        "11.\t**WindDir3pm** - arah angin jam 3 sore\n",
        "12.\t**WindSpeed9am** - kecepatan angin jam 9 pagi dalam km/jam dihitung dari rata-rata kecepatan angin 10 menit sebelum jam 3 sore\n",
        "13.\t**WindSpeed3pm** - kecepatan angin jam 3 sore dalam km/jam dihitung dari rata-rata kecepatan angin 10 menit sebelum jam 3 sore\n",
        "14.\t**Humidity9am** - humiditas jam 9 pagi dalam persen\n",
        "15.\t**Humidity3pm** - humiditas jam 3 sore dalam persen\n",
        "16.\t**Pressure9am** - tekanan udara jam 9 pagi dalam hpa\n",
        "17.\t**Pressure3pm** - tekanan udara jam 3 sore dalam hpa\n",
        "18.\t**Cloud9am** - persentase langit yang tertutup awan jam 9 pagi. dihitung dalam oktas, unit ⅛, menghitung berapa unit ⅛ dari langit yang tertutup awan. Jika 0, langit cerah, jika 8, langit sepenuhnya tertutup awan.\n",
        "19.\t**Cloud3pm** - persentase langit yang tertutup awan jam 3 sore\n",
        "20.\t**Temp9am** - temperatur jam 9 pagi dalam celcius\n",
        "21.\t**Temp3pm** - temperatur jam 3 sore dalam celcius\n",
        "22.\t**RainToday** - apakah hari ini hujan: jika curah hujan 24 jam sebelum jam 9 pagi melebihi 1mm, maka nilai ini adalah 1, jika tidak nilai nya 0\n",
        "23.\t**RainTomorrow** - variable yang mau di prediksi\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU3d_9-MbEdq"
      },
      "source": [
        "### 3.6 Melihat Statistik Dataframe secara umum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4tLUnlv-hNt"
      },
      "source": [
        "Karena dataset ini memiliki 2 tipe data yaitu float, dan object maka statistik ditampilkan per tipe data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SytUYdzM_oIm"
      },
      "source": [
        "#### 3.6.1 Mengecek statistik deskriptif dari masing-masing atribut yang bertipe float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez1sUXRQbFmt"
      },
      "outputs": [],
      "source": [
        "df.describe(include='float' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqy7U81pbbfO"
      },
      "source": [
        "#### 3.6.2 Mengecek statistik deskriptif dari masing-masing atribut yang bertipe objek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48CprtKmbdNE"
      },
      "outputs": [],
      "source": [
        "df.describe(include='O')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RY3nm4hbg8l"
      },
      "source": [
        "### 3.7 Melihat banyaknya unique values untuk tiap atribut."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVx7_EYqbi_Q"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "    print(col,': ', df[col].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7dIF_WbTSlV"
      },
      "source": [
        "## 4. Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79hlafv9BYrl"
      },
      "source": [
        "### 4.1 Mengecek apakah terdapat duplikat value pada dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPc60xkrTakq"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyhtysY0BrLQ"
      },
      "source": [
        "### 4.2 Menampilkan jumlah serta persentase dari missing value yang terdapat pada dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXix0mp8CDCu"
      },
      "outputs": [],
      "source": [
        "mv = pd.DataFrame({\"Missing Value\":df.isna().sum(), \"Persentase\":df.apply(lambda x: f'{((x.isnull().sum()/df.shape[0])*100).round(2)} %')})\n",
        "mv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VowM0XTBtN1M"
      },
      "source": [
        "### 4.3 Drop Attribut yang memiliki persentase missing value diatas 35 persen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ClCu7A9UeC7"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['Evaporation','Sunshine','Cloud9am','Cloud3pm'],inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMq5DgHuyFhU"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfZFlAFqyEaa"
      },
      "source": [
        "Dapat terlihat bahwa jumlah attribut yang ada berkurang dari 23 menjadi 19 buah."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDpig1e1uNoD"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jATwcFJyx60M"
      },
      "source": [
        "### 4.4 Mengisi Missing Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkn1WEqizbas"
      },
      "outputs": [],
      "source": [
        "df1=df.copy()\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FloiNBxq53G2"
      },
      "source": [
        "#### 4.4.1 Mengisi missing value pada attribut beripe data categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXg2eZJiHzMn"
      },
      "source": [
        "Ringkasan variabel kategori\n",
        "\n",
        "- Ada variabel tanggal. Itu dilambangkan dengan kolom `Tanggal`.\n",
        "\n",
        "- Ada 6 variabel kategori. Ini diberikan oleh `Location`, `WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` dan `RainTomorrow`.\n",
        "\n",
        "- Ada dua variabel kategorikal boolean - `RainToday` dan `RainTomorrow`.\n",
        "\n",
        "- `RainTomorrow` adalah variabel target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkAypkps3wc1"
      },
      "outputs": [],
      "source": [
        "categorical_cols = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n",
        "for col in categorical_cols:\n",
        "    df1[col] = df1[col].astype('category')\n",
        "\n",
        "# fill missing values with backward forward filling\n",
        "df1[categorical_cols] = df1[categorical_cols].fillna(method='bfill').fillna(method='ffill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGcXD82z8lzZ"
      },
      "outputs": [],
      "source": [
        "df1[categorical_cols].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8rQr8of1OFz"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx61n8vz9FR5"
      },
      "source": [
        "Dapat terlihat bahwa tidak lagi missing value pada attribut yang memiliki tipe data categorical setelah melakukan pengisian menggunakan metode backward forward filling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55LZw4i4-ee3"
      },
      "source": [
        "#### 4.4.2 Mengisi missing value pada attribut yang memiliki tipe data numerik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn2XV-k6_2ho"
      },
      "source": [
        "\n",
        "Untuk mengisi data yang hilang pada fitur numerik, dapat digunakan nilai rata-rata (mean) atau nilai tengah (median). Namun, harus diingat bahwa nilai rata-rata dapat dipengaruhi oleh nilai yang sangat jauh dari mayoritas data (outlier), sedangkan nilai tengah tidak dipengaruhi oleh outlier. Oleh karena itu, jika ingin memilih untuk mengisi nilai yang hilang dengan nilai rata-rata, pastikan bahwa outlier pada fitur numerik sudah ditangani dengan benar terlebih dahulu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KDcjeJHE4sy"
      },
      "outputs": [],
      "source": [
        "# Megetahui kolom yang memiliki tipe data numerik\n",
        "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "print(num_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGVdjqIj_ftg"
      },
      "outputs": [],
      "source": [
        "# Mengisi missing values menggunakan median\n",
        "plt.show()\n",
        "df1[num_cols].boxplot(figsize=(15,15), vert=False, color='purple')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOgwb1ncXz00"
      },
      "source": [
        "Dikarenakan masih banyak outlier sehingga kita harus menghilangkannya terlebih dahulu menggunakan metode IQR Outlier Removal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEA_oP1rZNoU"
      },
      "outputs": [],
      "source": [
        "outliers_data = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm']\n",
        "for data in outliers_data:\n",
        "    q1 = df1[data].quantile(0.25)\n",
        "    q3 = df1[data].quantile(0.75)\n",
        "    IQR = q3 - q1\n",
        "    lower_limit = q1 - (IQR * 1.5)\n",
        "    upper_limit = q3 + (IQR * 1.5)\n",
        "    df1.loc[df1[data] < lower_limit, data] = lower_limit\n",
        "    df1.loc[df1[data] > upper_limit, data] = upper_limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLMeE8LaZvpB"
      },
      "outputs": [],
      "source": [
        "df1[num_cols].boxplot(figsize=(30,30), vert=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tz-PiTDaI1w"
      },
      "source": [
        "Karena outliers telah ditangani maka kita dapat menggunakan mean untuk mengisi missing value pada data numerikal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aywndWVcdEkM"
      },
      "outputs": [],
      "source": [
        "for col in num_cols:\n",
        "    if df1[col].isnull().sum() > 0:\n",
        "        mean_value = df[col].mean()\n",
        "        df1[col].fillna(mean_value, inplace=True)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bQUDZZNeBYj"
      },
      "outputs": [],
      "source": [
        "df1[num_cols].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avxrnxN0gTO1"
      },
      "source": [
        "Pengisian missing value untuk data numerik telah berhasil dilakukan menggunakan mean, karna pada awalnya terdapat banyak outliers maka harus dihilangkan terlebih menggunakan metode IQR Outlier Removal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxuoxWfNwxno"
      },
      "source": [
        "### 4.5 Mengubah kolom 'Date' menjadi tipe data datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiVfMR-Rzybm"
      },
      "outputs": [],
      "source": [
        "df1['Date'] = pd.to_datetime(df['Date'])\n",
        "df1['Day'] = df1['Date'].dt.day\n",
        "df1['Month'] = df1['Date'].dt.month\n",
        "df1['Year'] = df1['Date'].dt.year\n",
        "\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B6QxSWH_-FH"
      },
      "source": [
        "\n",
        "Mengubah kolom 'date' menjadi tipe data datetime merupakan langkah penting dalam melakukan analisis data, karena kolom tersebut menyimpan informasi tanggal dan waktu. Dengan mengubah tipe data kolom 'date' menjadi datetime, kita dapat dengan mudah memanipulasi data tanggal dan waktu untuk menjawab pertanyaan analisis yang berkaitan dengan tren, musiman, dan pola data. Selain itu, dengan tipe data datetime, kita juga dapat melakukan agregasi data berdasarkan tanggal dan waktu seperti mingguan, bulanan, dan tahunan. Hal ini akan memudahkan kita dalam melakukan pemodelan dan membuat prediksi yang akurat berdasarkan data historis yang ada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6KPm9B__8yP"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FulRAQc2uinA"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVm3KgL039V9"
      },
      "source": [
        "## **5. Explorasi Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xypkatLUuinB"
      },
      "outputs": [],
      "source": [
        "df2 = df1.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVK-CXgl3PMS"
      },
      "outputs": [],
      "source": [
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i12CtFv4uinB"
      },
      "source": [
        "### 5.1 Data Query and Grouping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSEV56E6uinC"
      },
      "source": [
        "#### 5.1.1 Melakukan Query untuk Mendapatkan Data pada kota Perth pada Bulan Januari, dan Februari Tahun 2010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2pIb3jzuinC"
      },
      "outputs": [],
      "source": [
        "df_query = df2.query(\"Location=='Perth' & Date > '2010-01-01' & Date < '2010-03-01'\")\n",
        "df_query\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv3qa-_MuinC"
      },
      "source": [
        "#### 5.1.2 Mengelompokkan dan Menjumlahkan rata rata Suhu Tertinggi per Tahun di Setiap Lokasi sejak 2010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aan9tT6uinC"
      },
      "outputs": [],
      "source": [
        "# Data Grouping\n",
        "df_group = df2[(df2['Year'] >= 2010)].groupby(['Location','Year'])['MaxTemp'].mean()\n",
        "df_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1utgYtmyuinC"
      },
      "source": [
        "### 5.2 Central Tendency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yojDpmvuinD"
      },
      "source": [
        "#### 5.2.1 Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1tsCwjIuinD"
      },
      "outputs": [],
      "source": [
        "# Menemukan waktu rata-rata dari kolom Rainfall\n",
        "mean = df2['MinTemp'].mean()\n",
        "print(mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeGaWyKzuinD"
      },
      "source": [
        "#### 5.2.2 Modus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TMaR6rNiuinD"
      },
      "outputs": [],
      "source": [
        "# Menemukan modus dari arah angin pada daerah australia\n",
        "modus = df2['WindGustDir'].mode()\n",
        "print(modus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNKOxac9uinD"
      },
      "source": [
        "#### 5.2.3 Median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNcwVmIBuinE"
      },
      "outputs": [],
      "source": [
        "median = statistics.median(df2['Pressure9am'])\n",
        "print(median)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpRD6_DQuinE"
      },
      "source": [
        "Dapat dilihat beberapa nilai statistik dasar dari beberapa kolom pada dataset. Nilai mean atau rata-rata dari kolom MinTemp adalah 12.18640057012894. Sedangkan, untuk mencari modus dari kolom WindGustDir  menghasilkan output 'SSE' yang berarti pada daerah australia angin lebih sering datang dari arah selatan tenggara. Terakhir, median atau nilai tengah dari kolom Pressure9am adalah 1017.6. Informasi ini dapat membantu kita untuk memahami distribusi data pada dataset dan membentuk asumsi awal dalam membuat model machine learning yang lebih akurat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSzATvCDuinE"
      },
      "source": [
        "### 5.3 Variability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VPQDDUQuinE"
      },
      "outputs": [],
      "source": [
        "# Mencari variabilitas pada kolom 'MinTemp'\n",
        "variance = np.var(df2['MinTemp'])\n",
        "std_dev = np.std(df2['MinTemp'])\n",
        "z = df2['MinTemp']\n",
        "z.skew()\n",
        "percentile = np.percentile(z, [25, 50, 75])\n",
        "jangkauan = np.ptp(z)\n",
        "\n",
        "print(\"Variansi kolom MinTemp:\", variance)\n",
        "print(\"Standar deviasi kolom MinTemp:\", std_dev)\n",
        "print(\"Skewness: \", z.skew())\n",
        "print(\"Percentile: \", percentile)\n",
        "print(\"Range: \", jangkauan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuqeY5yiuinE"
      },
      "source": [
        "Dari analisa variabilitas kolom 'MinTemp' pada dataset, ditemukan nilai variansinya yang cukup besar sebesar 40.51 dan standar deviasi sebesar 6.36. Meskipun demikian, distribusi data cenderung normal dengan nilai skewness sebesar 0.02. Hasil persentil menunjukkan bahwa sekitar 50% data memiliki nilai suhu minimum antara 7.7 dan 12.1. Rentang nilai suhu minimum pada dataset adalah 37.2, menunjukkan perbedaan yang signifikan antara suhu minimum tertinggi dan terendah pada setiap lokasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQI2Kg6luinF"
      },
      "source": [
        "### 5.4 Visualisasi Sederhana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOrmXS6UuinF"
      },
      "source": [
        "#### 5.4.1 Intensitas Curah Hujan Wilayah Cairns per Tahun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56UX8ghbuinF"
      },
      "outputs": [],
      "source": [
        "df_cairns = df2[df2['Location'] == 'Cairns']\n",
        "\n",
        "# Group data berdasarkan tahun dan menghitung rata-rata curah hujan\n",
        "df_cairns_yearly = df_cairns.groupby(df_cairns['Date'].dt.year)['Rainfall'].mean().reset_index()\n",
        "print(df_cairns_yearly)\n",
        "\n",
        "# Plot line chart\n",
        "plt.plot(df_cairns_yearly['Date'], df_cairns_yearly['Rainfall'])\n",
        "plt.title('Intensitas Curah Hujan Wilayah Cairns per Tahun')\n",
        "plt.xlabel('Tahun')\n",
        "plt.ylabel('Curah Hujan (mm)')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlMLKBHauinF"
      },
      "source": [
        "\n",
        "Dari hasil plot line 'Intensitas Curah Hujan Wilayah Cairns per Tahun', dapat dilihat bahwa intensitas curah hujan di wilayah tersebut cenderung stabil dari tahun ke tahun, namun terdapat sedikit fluktuasi di beberapa tahun tertentu. Pada tahun 2017, terjadi peningkatan yang signifikan dalam intensitas curah hujan dibandingkan dengan tahun-tahun sebelumnya. Penurunan atau kenaikan intensitas curah hujan dapat menjadi indikasi adanya perubahan cuaca atau iklim di wilayah tersebut. Selain itu, informasi penting juga dapat diperoleh dari rata-rata intensitas curah hujan yang cukup tinggi, yaitu sekitar 0.7-0.9, yang dapat berdampak pada sektor pertanian atau pariwisata di wilayah Cairns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXPNQcw6uinG"
      },
      "source": [
        "#### 5.4.2 Jumlah Hari Terjadi Hujan per Wilayah Selama 2008-2017"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMePupoWuinG"
      },
      "outputs": [],
      "source": [
        "# subset data hanya untuk hari hujan\n",
        "df_rain = df2[df2['RainToday']=='Yes']\n",
        "\n",
        "# hitung jumlah hari hujan per wilayah\n",
        "df_rain_days = df_rain.groupby('Location')['RainToday'].count().reset_index(name='Hari Hujan')\n",
        "print(df_rain_days)\n",
        "\n",
        "# buat visualisasi\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Location', y='Hari Hujan', data=df_rain_days)\n",
        "plt.title('Jumlah Hari Hujan per Wilayah Selama 2008-2017')\n",
        "plt.xlabel('Wilayah')\n",
        "plt.ylabel('Jumlah Hari Hujan')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGa_DEq9uinG"
      },
      "source": [
        "Dapat dilihat bahwa wilayah Cairns memiliki jumlah hari hujan tertinggi di antara semua wilayah dengan total 980 hari hujan, diikuti oleh Williamtown dengan 1055 hari hujan, dan Portland dengan 1094 hari hujan. Sementara itu, wilayah Uluru memiliki jumlah hari hujan yang paling sedikit, yaitu hanya 117 hari hujan selama sepuluh tahun terakhir.\n",
        "\n",
        "Perlu diperhatikan bahwa data yang digunakan adalah jumlah hari hujan selama periode 2008-2017, bukan jumlah hari hujan pertahun. Meskipun demikian, rata-rata jumlah hari hujan di semua wilayah tergolong cukup tinggi, yaitu berkisar antara 500 hingga 1000 hari hujan dalam sepuluh tahun terakhir. Hal ini menunjukkan bahwa cuaca di Australia cenderung basah dan lembab di banyak wilayahnya, yang dapat memengaruhi sektor pertanian, pariwisata, dan infrastruktur di daerah-daerah tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIl6vGD4uinG"
      },
      "source": [
        "#### 5.4.3 Distribusi Arah Angin di Australia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDB6jr3puinH"
      },
      "outputs": [],
      "source": [
        "wind_gust_dir = df2['WindGustDir'].value_counts()\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(wind_gust_dir, labels=wind_gust_dir.index, autopct='%1.1f%%')\n",
        "plt.title('Pie Chart Arah Angin di Australia')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGqRosXPuinH"
      },
      "source": [
        "Pie chart di atas menampilkan distribusi arah angin (WindGustDir) di Australia selama 2008-2017. Dapat dilihat bahwa angin dari arah SSE merupakan arah angin yang paling sering terjadi dengan jumlah mencapai 9,2%. Sedangkan arah angin terendah adalah NNW dan NNE yang hanya mencapai 4,6% dari total seluruh arah angin. Hal ini menunjukkan bahwa arah angin SSE lebih sering terjadi di Australia dibandingkan dengan arah angin lainnya. Arah angin yang lainnya juga terdistribusi cukup merata dan memiliki jumlah yang hampir sama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnuuzFjFuinI"
      },
      "source": [
        "#### 5.4.4 Korelasi Antar Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umhXcE1SLlOw"
      },
      "source": [
        "Membuat gambar (figure) dengan ukuran 18 x 12 inci menggunakan library matplotlib.pyplot (plt). Di dalam gambar tersebut, ditampilkan sebuah peta panas (heatmap) yang merepresentasikan korelasi antara variabel-variabel dalam dataframe (df_corr) dengan rotasi 90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GVMcDwwLq-L"
      },
      "outputs": [],
      "source": [
        "heatmap_data = df2.drop(['Year', 'Month', 'Day'], axis=1)\n",
        "plt.figure(figsize=(18,12))\n",
        "sns.heatmap(heatmap_data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKHTi2hxLu_f"
      },
      "source": [
        "Berdasarkan heatmap korelasi pada dataset yang diberikan, terlihat bahwa terdapat korelasi positif yang cukup tinggi antara RainToday (apakah hujan hari ini) dan Humidity3pm (kelembaban relatif udara pada pukul 3 sore), yaitu sekitar 0.45. Hal ini menunjukkan bahwa jika kelembaban relatif udara pada pukul 3 sore tinggi, kemungkinan besar akan terjadi hujan pada hari itu.\n",
        "\n",
        "Selain itu, hasil heatmap juga menunjukkan bahwa kecepatan angin pada pukul 3 sore (WindSpeed3pm) tidak terlalu dipengaruhi oleh tekanan udara di pagi atau sore hari sebelumnya (Pressure9am dan Pressure3pm) karena korelasinya cukup lemah."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1kyZkh9L0et"
      },
      "source": [
        "#### 5.4.5 WindGustDir, WindDir9am, dan WindDir3pm dengan Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAqeTKSPMN1a"
      },
      "outputs": [],
      "source": [
        "dff = df2.copy()\n",
        "cat = ['WindGustDir','WindDir9am','WindDir3pm','RainTomorrow']\n",
        "le =  LabelEncoder()\n",
        "for i in cat:\n",
        "        dff[i] = le.fit_transform(dff[i])\n",
        "\n",
        "dff = dff[['WindGustDir','WindDir9am','WindDir3pm','RainTomorrow']]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(dff.corr(), annot=True)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXrTcLZuMSKx"
      },
      "source": [
        "Berdasarkan heatmap korelasi diatas, terlihat bahwa atribut WindGustDir memiliki korelasi positif yang lemah dengan kemungkinan terjadinya hujan pada esok hari. Dalam arti lain, jika arah angin pada saat gust memiliki arah tertentu, kemungkinan terjadinya hujan pada esok hari lebih besar. Namun, untuk atribut WindDir9am dan WindDir3pm, korelasinya dengan RainTomorrow sangat kecil. Hal ini menunjukkan bahwa arah angin pada pukul 9 pagi atau pukul 3 sore tidak berpengaruh signifikan pada kemungkinan terjadinya hujan pada esok hari. Analisa ini menggunakan RainTomorrow sebagai target variabel yang ingin diprediksi di masa yang akan datang, sedangkan atribut lain seperti WindGustDir, WindDir9am, dan WindDir3pm digunakan sebagai fitur variabel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEd-Gu2zM7sI"
      },
      "source": [
        "## **6. Pre Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FStQsyHbuinM"
      },
      "outputs": [],
      "source": [
        "df3=df2.copy()\n",
        "df3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ujsCT4uinM"
      },
      "source": [
        "### 6.1 Encode Untuk Tipe Data Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOEApj3buinN"
      },
      "outputs": [],
      "source": [
        "df3['Location'] = le.fit_transform(df3['Location'])\n",
        "df3['WindGustDir'] = le.fit_transform(df3['WindGustDir'])\n",
        "df3['WindDir9am'] = le.fit_transform(df3['WindDir9am'])\n",
        "df3['WindDir3pm'] = le.fit_transform(df3['WindDir3pm'])\n",
        "df3['RainToday'] = le.fit_transform(df3['RainToday'])\n",
        "df3['RainTomorrow'] = le.fit_transform(df3['RainTomorrow'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z07jIni0uinN"
      },
      "outputs": [],
      "source": [
        "df3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG32kxHAuinN"
      },
      "source": [
        "Kolom yang memiliki tipe data categorial telah berhasil diubah menjadi nilai numerik melalui label encoder. Dimana tujuan dilakukan ini adalah untuk mempermudah melakukan pemodelan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y5aNPFXNHpi"
      },
      "source": [
        "### 6.2 Scaling Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gELmB-bNPBQ"
      },
      "outputs": [],
      "source": [
        "num_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
        "    'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrZXTjzsQEbW"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df3[num_cols] = scaler.fit_transform(df3[num_cols])\n",
        "df3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kA5W_9qQGpp"
      },
      "source": [
        "\n",
        "MinMaxScaler merupakan teknik normalisasi data yang digunakan untuk mengubah rentang nilai suatu fitur menjadi rentang nilai yang diinginkan, seperti rentang [0,1]. Normalisasi dilakukan dengan membagi setiap nilai pada fitur dengan selisih nilai maksimum dan minimum dari fitur tersebut. Pada dataset di atas, normalisasi menggunakan MinMaxScaler lebih tepat karena beberapa fitur memiliki rentang nilai yang berbeda, sehingga normalisasi dapat memastikan bahwa setiap fitur memiliki bobot yang sama pentingnya dalam model. Normalisasi juga dapat meningkatkan performa algoritma machine learning, terutama pada algoritma yang sensitif terhadap skala data seperti KNN atau SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxIFy0vyuinP"
      },
      "outputs": [],
      "source": [
        "df3.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPUo5JOwuinQ"
      },
      "source": [
        "### 6.3 Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kufyvi-0uinQ"
      },
      "outputs": [],
      "source": [
        "# Membuang kolom yang tidak diperlukan\n",
        "df3 = df3.drop(['Date', 'Location', 'WindGustDir', 'WindDir9am'], axis=1)\n",
        "\n",
        "# Menghitung korelasi antara setiap atribut dengan kolom target 'RainTomorrow'\n",
        "corr_matrix = df3.corr()\n",
        "corr_target = corr_matrix['RainTomorrow']\n",
        "\n",
        "# Membuat dataframe korelasi antara semua atribut terhadap kolom 'RainTomorrow'\n",
        "df_corr = pd.DataFrame({'attribute':corr_target.index, 'correlation':corr_target.values})\n",
        "df_corr = df_corr.sort_values(by='correlation', ascending=False)\n",
        "\n",
        "# Menampilkan dataframe korelasi antara semua atribut terhadap kolom 'RainTomorrow'\n",
        "print(df_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArNsowIUuinQ"
      },
      "outputs": [],
      "source": [
        "low_corr_cols = corr_target[corr_target < 0.1].index.values\n",
        "df3 = df3.drop(low_corr_cols, axis=1)\n",
        "print(df3.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLLAi5FUuinQ"
      },
      "source": [
        "Sebelum Pendefinisian model, ada baiknya kita mengecek kembali apakah kolom-kolom yang ada itu dapat berguna untuk pembuatan model, karena efisiensi kerja model dipengaruhi oleh jumlah data yang diolah. Jadi dapat kita lihat bahwa kolom yang memiliki korelasi baik dengan variable target yaitu \"RainTomorrow\" itu hanya \"Rainfall\", \"WindGustSpeed\", \"Humidity9am\", \"Humidity3pm\", \"RainToday\", sehingga kolom kolom lain harus di drop karna memiliki nilai korelasi yang sama dengan 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqcuOGteRqtS"
      },
      "source": [
        "## **7.Pendefinisian Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6KAzxUNuinR"
      },
      "outputs": [],
      "source": [
        "df4=df3.copy()\n",
        "df4.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgE2oKpxuinR"
      },
      "source": [
        "Dilakukan Pembagian data untuk pembuatan model, dimana data training akan memiliki 70% dari jumlah data dan data testing memiliki 30% dari jumlah data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF_FzBkRuinR"
      },
      "outputs": [],
      "source": [
        "# Pisahkan fitur dan label\n",
        "X = df4.drop(['RainTomorrow'], axis=1)\n",
        "y = df4['RainTomorrow']\n",
        "\n",
        "# Pisahkan df4 menjadi df4 latih dan df4 uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Cetak jumlah df4 pada masing-masing set\n",
        "print(\"Jumlah data pada X_train: \", len(X_train))\n",
        "print(\"Jumlah data pada y_train: \", len(y_train))\n",
        "print(\"Jumlah data pada X_test: \", len(X_test))\n",
        "print(\"Jumlah data pada y_test: \", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuWRsYEKuinR"
      },
      "source": [
        "Jenis algortima yang akan dipakai ada 5 buah yaitu Logistic Regression, Decission Tree, Random Forest, Naive Baye, dan KNN. Yang kemudian akan di evaluasi dengan confusion matrix untuk menampilkan hasil prediksi dan aktual untuk masing-masing class, serta classification report untuk melihat akurasi yang dihasilkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxzmx677uinS"
      },
      "source": [
        "### 7.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO0jp_kDuinS"
      },
      "outputs": [],
      "source": [
        "# Membuat model Logistic Regression dan melatihnya dengan training set\n",
        "model_lr = LogisticRegression(random_state=0, solver='liblinear')\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "# Memprediksi test set menggunakan model yang sudah dilatih\n",
        "y_pred_lr = model_lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZoORNGjuinS"
      },
      "outputs": [],
      "source": [
        "# Melihat nilai akurasi\n",
        "print('Train Score: {:.4f}'.format(model_lr.score(X_train, y_train)))\n",
        "print('Test Score:  {:.4f}'.format(model_lr.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X_NkG_euinS"
      },
      "outputs": [],
      "source": [
        "# Membuat confusion matrix\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Nilai Sebenarnya')\n",
        "plt.xticks([0.5, 1.5], ['Predicted 0', 'Predicted 1'])\n",
        "plt.yticks([0.5, 1.5], ['Actual 0', 'Actual 1'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H6Lo7ehuinT"
      },
      "source": [
        "Confusion Matrix Logistic Regression menunjukkan 31.595 + 4.513 = 36.108 prediksi benar dan 5.684 + 1.846  = 7530 prediksi salah. Hasil confusion matrix menunjukkan bahwa model memiliki kecenderungan untuk memprediksi dengan baik kelas mayoritas (\"NotRaintomorrow\") tetapi mengalami kesulitan dalam memprediksi kelas minoritas (\"RainTomorrow\"). Terdapat jumlah false positive yang cukup tinggi, yang berarti model cenderung salah memprediksi cuaca NotRaintomorrow. Penyempurnaan perlu dilakukan untuk meningkatkan kinerja model dalam memprediksi cuaca RainTomorrow agar lebih seimbang dan akurat.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxW6sIt5uinT"
      },
      "outputs": [],
      "source": [
        "# Membuat classification report\n",
        "print(classification_report(y_test, y_pred_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiUOmY4luinT"
      },
      "source": [
        "\n",
        "Dari hasil analisis, dapat disimpulkan bahwa model Logistic Regression memiliki tingkat keakuratan yang baik dalam memprediksi kelas \"NotRainTomorrow\" (kelas 0) dengan tingkat kebenaran sekitar 85%. Namun, model ini cenderung kurang akurat dalam memprediksi kelas \"RainTomorrow\" (kelas 1) dengan tingkat kebenaran sekitar 71%. Dalam hal mengidentifikasi data yang sebenarnya adalah \"NotRainTomorrow\", model memiliki tingkat keberhasilan sekitar 94%, sementara dalam mengidentifikasi data yang sebenarnya adalah \"RainTomorrow\", tingkat keberhasilannya hanya sekitar 44%. Secara keseluruhan, model Logistic Regression memiliki kinerja yang baik dengan akurasi sekitar 83%, tetapi perlu diperhatikan kinerja model dalam memprediksi kelas minoritas (RainTomorrow)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMk5pQRCuinU"
      },
      "source": [
        "### 7.2 Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yey_oEr9uinU"
      },
      "outputs": [],
      "source": [
        "# Membuat model SVM dan melatihnya dengan training set\n",
        "model_svm = SVC(random_state=0)\n",
        "model_svm.fit(X_train, y_train)\n",
        "\n",
        "# Memprediksi test set menggunakan model yang sudah dilatih\n",
        "y_pred_svm = model_svm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRrik7mNuinU"
      },
      "outputs": [],
      "source": [
        "# Melihat nilai akurasi\n",
        "print('Train Score: {:.4f}'.format(model_svm.score(X_train, y_train)))\n",
        "print('Test Score:  {:.4f}'.format(model_svm.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rj5azsLuinU"
      },
      "outputs": [],
      "source": [
        "# Membuat confusion matrix\n",
        "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Oranges')\n",
        "plt.title('Confusion Matrix - Support Vector Machine')\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Nilai Sebenarnya')\n",
        "plt.xticks([0.5, 1.5], ['Predicted 0', 'Predicted 1'])\n",
        "plt.yticks([0.5, 1.5], ['Actual 0', 'Actual 1'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwoSenjHuinV"
      },
      "source": [
        "Confusion Matrix Support Vector Machine menunjukkan 32244 + 4323 = 36567 prediksi benar dan 5874 + 1197 = 7071 prediksi salah. Berdasarkan hasil confusion matrix dari algoritma SVM, dapat disimpulkan bahwa model ini memiliki performa yang baik dalam memprediksi kelas 0 (\"tidak hujan besok\"), namun masih kesulitan dalam memprediksi kelas 1 (\"hujan besok\"). Model ini cenderung memprediksi cuaca tidak hujan padahal sebenarnya akan hujan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQI1I8KeuinV"
      },
      "outputs": [],
      "source": [
        "# Membuat classification report\n",
        "print(classification_report(y_test, y_pred_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK3ewkMEuinV"
      },
      "source": [
        "Model memiliki presisi (precision) sebesar 0.85 untuk kelas 0 dan 0.78 untuk kelas 1, yang berarti sekitar 85% data prediksi kelas 0 dan 78% data prediksi kelas 1 benar-benar sesuai dengan kelas aktualnya. Recall (sensitivitas) untuk kelas 0 adalah 0.96, menunjukkan bahwa sekitar 96% data kelas 0 berhasil dideteksi dengan benar, namun recall untuk kelas 1 hanya sekitar 42%. F1-score untuk kelas 0 adalah 0.90, sedangkan untuk kelas 1 adalah 0.55, menunjukkan performa yang lebih baik dalam memprediksi kelas 0. Akurasi model adalah 0.84, artinya sekitar 84% data berhasil diprediksi dengan benar. Meskipun model memiliki akurasi yang tinggi, performa dalam memprediksi kelas minoritas (kelas 1) masih perlu ditingkatkan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQvENR0RuinV"
      },
      "source": [
        "### 7.3 Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYWW_o6UuinV"
      },
      "outputs": [],
      "source": [
        "# Membuat model Naive Bayes dan melatihnya dengan training set\n",
        "model_nb = GaussianNB()\n",
        "model_nb.fit(X_train, y_train)\n",
        "\n",
        "# Memprediksi test set menggunakan model yang sudah dilatih\n",
        "y_pred_nb = model_nb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2WnW84YuinW"
      },
      "outputs": [],
      "source": [
        "# Melihat nilai akurasi\n",
        "print('Train Score: {:.4f}'.format(model_nb.score(X_train, y_train)))\n",
        "print('Test Score:  {:.4f}'.format(model_nb.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYANxsGGuinW"
      },
      "outputs": [],
      "source": [
        "# Membuat confusion matrix\n",
        "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title('Confusion Matrix - Naive Bayes')\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Nilai Sebenarnya')\n",
        "plt.xticks([0.5, 1.5], ['Predicted 0', 'Predicted 1'])\n",
        "plt.yticks([0.5, 1.5], ['Actual 0', 'Actual 1'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awz9jHEguinW"
      },
      "source": [
        "\n",
        "Model Naive Bayes memprediksi 28.096 data dengan benar sebagai kelas 0 dan juga benar secara aktual. Namun, terdapat 5.345 data yang diprediksi sebagai kelas 1 padahal sebenarnya kelasnya adalah 0. Selain itu, terdapat 4.625 data yang diprediksi sebagai kelas 0 padahal sebenarnya kelasnya adalah 1. Model memiliki performa yang baik dalam memprediksi kelas 0, namun mengalami kesulitan dalam memprediksi kelas 1 dengan jumlah false positive yang tinggi. Hal ini menunjukkan kecenderungan model untuk memprediksi cuaca tidak hujan padahal sebenarnya cuaca akan hujan. Diperlukan analisis lebih lanjut dan penyesuaian pada model Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwl3CaxCuinX"
      },
      "outputs": [],
      "source": [
        "# Membuat classification report\n",
        "print(classification_report(y_test, y_pred_nb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njbb1eRcuinX"
      },
      "source": [
        "Model Naive Bayes memiliki precision yang baik untuk kelas mayoritas (kelas 0) sekitar 86%, namun performanya rendah untuk kelas minoritas (kelas 1) dengan precision sekitar 51%. Recall untuk kelas 0 adalah sekitar 84%, sementara untuk kelas 1 adalah sekitar 55%. F1-score untuk kelas 0 adalah 0.85, dan untuk kelas 1 adalah 0.53. Akurasinya mencapai 77%. Model ini lebih baik dalam memprediksi kelas mayoritas dan memerlukan optimasi untuk meningkatkan prediksi kelas minoritas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzG0bQRUuinX"
      },
      "source": [
        "### 7.4 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tacBPi8kuinX"
      },
      "outputs": [],
      "source": [
        "# Membuat model Random Forest dan melatihnya dengan training set\n",
        "model_rf = RandomForestClassifier(random_state=0)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Memprediksi test set menggunakan model yang sudah dilatih\n",
        "y_pred_rf = model_rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5_BDh_yuinY"
      },
      "outputs": [],
      "source": [
        "# Melihat nilai akurasi\n",
        "print('Train Score: {:.4f}'.format(model_rf.score(X_train, y_train)))\n",
        "print('Test Score:  {:.4f}'.format(model_rf.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF2M1d0ZuinY"
      },
      "outputs": [],
      "source": [
        "# Membuat confusion matrix\n",
        "cm_nb = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Purples')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Nilai Sebenarnya')\n",
        "plt.xticks([0.5, 1.5], ['Predicted 0', 'Predicted 1'])\n",
        "plt.yticks([0.5, 1.5], ['Actual 0', 'Actual 1'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPwgv_RquinY"
      },
      "source": [
        "Model Random Forest memiliki performa yang baik dalam memprediksi kelas mayoritas (kelas 0) dengan jumlah true negative yang tinggi (30.399). Namun, model memiliki kesulitan dalam memprediksi kelas minoritas (kelas 1) dengan jumlah false positive yang cukup tinggi (3.042). Hal ini menunjukkan kecenderungan model untuk memprediksi cuaca tidak hujan (kelas 0) ketika sebenarnya cuaca akan hujan (kelas 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW1nQ9r3uinZ"
      },
      "outputs": [],
      "source": [
        "# Membuat classification report\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8WNaCXbuinZ"
      },
      "source": [
        "Dalam model Random Forest, sekitar 85% data yang diprediksi sebagai kelas 0 benar-benar kelas 0, sedangkan sekitar 62% data yang diprediksi sebagai kelas 1 adalah benar-benar kelas 1. Model memiliki sensitivitas yang baik dalam mendeteksi kelas 0 (91%), namun memiliki tingkat recall yang rendah untuk kelas 1 (49%). F1-score untuk kelas 0 adalah 0.88, sedangkan untuk kelas 1 adalah 0.55. Dengan akurasi sebesar 81%, model memiliki performa yang baik dalam memprediksi kelas mayoritas (kelas 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZIy-iB-uinZ"
      },
      "source": [
        "### 7.5 Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRHz2MQ3uinZ"
      },
      "outputs": [],
      "source": [
        "# Membuat model Decision Tree dan melatihnya dengan training set\n",
        "model_dt = DecisionTreeClassifier(random_state=0)\n",
        "model_dt.fit(X_train, y_train)\n",
        "\n",
        "# Memprediksi test set menggunakan model yang sudah dilatih\n",
        "y_pred_dt = model_dt.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSWLPA7JuinZ"
      },
      "outputs": [],
      "source": [
        "# Melihat nilai akurasi\n",
        "print('Train Score: {:.4f}'.format(model_dt.score(X_train, y_train)))\n",
        "print('Test Score:  {:.4f}'.format(model_dt.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB81Mu1uuinZ"
      },
      "outputs": [],
      "source": [
        "# Membuat confusion matrix\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Reds')\n",
        "plt.title('Confusion Matrix - Decision Tree')\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Nilai Sebenarnya')\n",
        "plt.xticks([0.5, 1.5], ['Predicted 0', 'Predicted 1'])\n",
        "plt.yticks([0.5, 1.5], ['Actual 0', 'Actual 1'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nDeJBgsuina"
      },
      "source": [
        "Confusion Matrix Logistic Regression menunjukkan 29018 + 4.918 = 33936 prediksi benar dan 4423 + 5279 = 9702 prediksi salah. Model Decision Tree memiliki performa yang baik dalam memprediksi kelas 0 (\"tidak hujan besok\"), dengan 29.018 data yang diprediksi benar sebagai kelas 0 secara aktual. Namun, model memiliki kesulitan dalam memprediksi kelas 1 (\"hujan besok\"), terlihat dari jumlah false positive yang mencapai 4.423 data. Meskipun model berhasil memprediksi 4.918 data dengan benar sebagai kelas 1, namun masih terdapat kekurangan dalam mendeteksi sebagian besar data kelas 1. Terdapat kecenderungan model untuk memprediksi cuaca tidak hujan (kelas 0) padahal sebenarnya cuaca akan hujan (kelas 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az0QL743uina"
      },
      "outputs": [],
      "source": [
        "# Membuat classification report\n",
        "print(classification_report(y_test, y_pred_dt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3m7r0duina"
      },
      "source": [
        "Model Decision Tree memiliki performa yang cukup baik dalam memprediksi kelas mayoritas (kelas 0) dengan presisi sekitar 85% dan recall sekitar 87%. Namun, model memiliki performa yang lebih rendah dalam memprediksi kelas minoritas (kelas 1) dengan presisi sekitar 53% dan recall sekitar 48%. Akurasi model mencapai 78%. Meskipun memiliki presisi dan recall yang tinggi untuk kelas 0, perlu dilakukan perbaikan untuk meningkatkan deteksi kelas 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGkeQS31uina"
      },
      "source": [
        "## **8. Kesimpulan**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGGQpVmBuina"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}